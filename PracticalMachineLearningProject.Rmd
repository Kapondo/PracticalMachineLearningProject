---
title: 'Practical Machine Learning Final Project: Prediction with Classification'
author: "Tongesai Kapondo"
date: "22 January 2016"
geometry: margin=1in
output:
  html_document:
    toc: yes
    keep_md: yes
---

##<span style="color:#1a53ff">Introduction</span>

### Background

Majority of the attention in human activity recognition research focuses on discrimination between different types of activities, but not quality of the activities. In this study, the goal is to investigate how well an activity was performed by six wearers of electronic devices. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways, namely

* __Class A:__ exactly according to the specification
* __Class B:__ throwing the elbows to the front
* __Class C:__ lifting the dumbbell only halfway
* __Class D:__ lowering the dumbbell only half way
* __Class E:__ throwing the hips to the front.

### Data Source

The data for this project is cited [here](http://groupware.les.inf.puc-rio.br/har).

The **training data** for this project are available here:
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The **testing data** are available here:
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

### Goal of Project

The goal of the project is to predict the manner in which the exercises were done. This is the **"classe"** variable in the training set. The created report describes how one builts a model, how to use cross validation, what one thinks the expected out of sample error is, and why you made the choices you did. There is also requirement to use your prediction model to predict 20 different test cases.

### Loading required libraries/packages

```{r}
library(pacman)
p_load("caret", "knitr", "randomForest", "rpart", "rpart.plot", "gbm", "rattle", "plyr", "ggplot2", "doParallel", "corrplot")
```


##<span style="color:#1a53ff">Getting and loading the data</span>

```{r, cache=TRUE}
set.seed(1234)

training.url <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testing.url <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

training <- read.csv(url(training.url), na.strings=c("NA","#DIV/0!",""))
testing <- read.csv(url(testing.url), na.strings=c("NA","#DIV/0!",""))
dim(training); dim(testing)
```

The raw dataset contained **`r nrow(training)`** rows of data, with **`r ncol(training)`** variables. Many variables contained largely missing data (usually with only one row of data), so these were removed from the dataset. In addition, variables not concerning the movement sensors were also removed.


##<span style="color:#1a53ff">Cleaning the data</span>

Remove variables that has more than 75% NAs.

```{r}
noNAs <- sapply(training, function(x) mean(is.na(x))) > 0.75
training <- training[, noNAs==FALSE]
noNAs <- sapply(testing, function(x) mean(is.na(x))) > 0.75
testing <- testing[, noNAs==FALSE]
dim(training); dim(testing)
```

Removing first 5 ID variables so that it does not interfer with `ML` Algorithms:

```{r}
# remove identification only variables (columns 1 to 5)
training <- training[, -(1:5)]; testing <- testing[, -(1:5)]
dim(training); dim(testing)
```
**Removing near zero variances**

Removing near zero variances using **`nearZeroVar`** function. `nearZeroVar` diagnoses predictors that have one unique value (i.e. are zero variance predictors) or predictors that are both of the following characteristics: they have very few unique values relative to the number of samples and the ratio of the frequency of the most common value to the frequency of the second most common value is large. `checkConditionalX` looks at the distribution of the columns of x conditioned on the levels of y and identifies columns of x that are sparse within groups of y.

```{r}
nzv <- nearZeroVar(training, saveMetrics=TRUE)
training <- training[,nzv$nzv==FALSE]

nzv<- nearZeroVar(testing,saveMetrics=TRUE)
testing <- testing[,nzv$nzv==FALSE]
dim(training); dim(testing)
```

Removing first column referred to as **num_window** , which has nothing to do with the exercises .

```{r}
training <- training[c(-1)]; testing <- testing[c(-1)]
dim(training); dim(testing)
```


After the cleaning process above, the number of variables for the analysis has been reduced to **`r ncol(training)`** only.

##<span style="color:#1a53ff">Partitioning the Data</span>

The dataset was partitioned into **training** and **testing** datasets, with `60%` of the original data going to the **training** set and `40%` to the **testing** set. The model was built with the **training dataset**, then tested on the **testing dataset**. The following code performs this procedure:

```{r}
inTrain <- createDataPartition(training$classe, p=0.6, list=FALSE)
myTraining <- training[inTrain, ]
myTesting <- training[-inTrain, ]
dim(myTraining); dim(myTesting)
```

## <span style="color:#1a53ff">Correlation</span>

```{r}
col1 <- colorRampPalette(c("#00007F" , "blue","#007FFF" , "cyan", "white",
                           "yellow", "#FF7F00", "red", "#7F0000"))
corMatrix <- cor(myTraining[, -53])
corrplot(corMatrix, order = "hclust", tl.cex =0.6,addrect =4, col = col1(50))
```

The highly correlated variables are shown in dark colors in the graph above. If using `"hclust"`, `corrplot()` can draw circles around the chart of correlation matrix based on the results of hierarchical clustering.


##<span style="color:#1a53ff">Model Predictions</span>

Many methods of classification were attempted, including **`Decision Trees`, `Random Forests`** and **`Generalized Boosted Regression (GBM)`**.

The R codes used are shown below, as are the confusion matrices. The `OOB` error rate in the training and the confusion matrix is shown below.

<br>

###<span style="color:#1a53ff">Prediction with Decision Trees</span> 

```{r}
set.seed(12345)
DTmodelFit <- rpart(classe ~ ., data=myTraining, method="class")
fancyRpartPlot(DTmodelFit, cex =0.4, ycompress =FALSE)
```

### Prediction
```{r}
DTpredictions <- predict(DTmodelFit, myTesting, type = "class")
DeciTree <- confusionMatrix(DTpredictions, myTesting$classe)
DeciTree
```
The accuracy of predicting with trees is **`r round(DeciTree$overall['Accuracy'],4)`** with **95% CI : (0.7294, 0.7489)**. 


###<span style="color:#1a53ff">Prediction with Random Forests</span> 

```{r}
set.seed(1777)
Random_Forest <- randomForest(classe ~. , data=myTraining)
RFpredictions <- predict(Random_Forest, myTesting, type = "class")
```

### Using Confusion Matrix to test results

```{r}
ConfMatRF <- confusionMatrix(RFpredictions, myTesting$classe); ConfMatRF

# Plotting Matrix Results

plot(ConfMatRF$table, col = ConfMatRF$byClass, 
main = paste("Random Forest: Accuracy =",round(ConfMatRF$overall['Accuracy'], 4)))
```

The Random Forest Algorithm resulted in predictions with an accuracy of **`r round(ConfMatRF$overall['Accuracy'], 4)`** and **95% CI : (0.9905, 0.9944)**. This is a pretty good accuracy!


## <span style="color:#1a53ff">Cross-validation</span>

For purpose of cross-validation - **"cv"**, I partitioned the training set into a smaller set called smalltraining (30% of data) to speed up the running of the model

```{r}
InTrain<-createDataPartition(y=training$classe,p=0.30,list=FALSE)
smalltraining<-training[InTrain,]
```

Here I used the caret package with Random Forest as my model with 5 fold cross validation - **"cv"**

```{r, cache=TRUE}
RF_model_small<-train(classe~.,data=smalltraining,method="rf",
                trControl=trainControl(method="cv",number=5),
                prox=TRUE,doParallel=TRUE)
RF_model_small; RF_model_small$finalModel
```

That is a pretty amazingly good model! **0.9783** accuracy!

**The Expected out-of-sample error**

The expected out-of-sample error is calculated as **1 - accuracy** for predictions made against the cross-validation set.The expected out-of-sample error is estimated at **`r (1- 0.9782629)`**, or **`r (1- 0.9782629)*100`%**.  With an accuracy above 95% on our cross-validation data, we can expect that very few, or none, of the test samples will be missclassified.


###<span style="color:#1a53ff"> Prediction with Generalized Boosted Regression (GBM)</span>

```{r, cache=TRUE}
set.seed(1234)
fitControl <- trainControl(method = "repeatedcv",
                           number = 5,
                           repeats = 1)

gbmFit <- train(classe ~ ., data=myTraining, method = "gbm",
                 trControl = fitControl,
                 verbose = FALSE)

gbmFinMod <- gbmFit$finalModel

gbmPredTest <- predict(gbmFit, newdata=myTesting)
gbmConfMat <- confusionMatrix(gbmPredTest, myTesting$classe)
gbmConfMat

# Plotting Matrix Results

plot(gbmConfMat$table, col = gbmConfMat$byClass, 
main = paste("GBM: Accuracy =",round(gbmConfMat$overall['Accuracy'], 4)))
```

The Generalized Boosted Regression Model (GBM) predictions resulted with an accuracy of **`r round(gbmConfMat$overall['Accuracy'], 4)`** and **95% CI : (0.9561, 0.9648)**

## <span style="color:#1a53ff">Conclusion</span>

* Decision Tree Accuracy =__`r round(DeciTree$overall['Accuracy'],4)`__
* Generalized Boosted(GBM) Accuracy = __`r round(gbmConfMat$overall['Accuracy'], 4)`__
* Random Forest Accuracy =__`r round(ConfMatRF$overall['Accuracy'], 4)`__


Predictions with the **Random Forest** algorithm provided the best results, compared to the other two algorithms tested, even with the smaller sample used for cross-validation which resulted in an accuracy of **0.9783**.